\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {section}{Introduction}{1}{section*.1}}
\@writefile{toc}{\contentsline {section}{Data integrity and preprocessing}{1}{section*.2}}
\@writefile{toc}{\contentsline {subsection}{Integrity}{1}{subsection*.3}}
\@writefile{toc}{\contentsline {subsection}{Preprocessing}{1}{subsection*.4}}
\@writefile{toc}{\contentsline {section}{The selection of relevant information}{1}{section*.5}}
\@writefile{toc}{\contentsline {subsection}{The specificity of text data}{2}{subsection*.6}}
\@writefile{toc}{\contentsline {paragraph}{Numerous features}{2}{paragraph*.7}}
\@writefile{toc}{\contentsline {paragraph}{Unbalanced data}{2}{paragraph*.8}}
\@writefile{toc}{\contentsline {paragraph}{Natural language and feature extraction}{2}{paragraph*.9}}
\@writefile{toc}{\contentsline {section}{Modelling}{2}{section*.10}}
\@writefile{toc}{\contentsline {subsection}{First approach: Naive Bayes}{2}{subsection*.11}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Test set confusion matrix of NB model fitted with unbalanced dataset.}}{3}{paragraph*.12}}
\newlabel{NB1unbalanced}{{1}{3}{Sampling}{paragraph*.12}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Test set confusion matrix of NB model fitted with balanced dataset.}}{3}{paragraph*.12}}
\newlabel{NB1balanced}{{2}{3}{Sampling}{paragraph*.12}{}}
\@writefile{toc}{\contentsline {paragraph}{Sampling}{3}{paragraph*.12}}
\newpmemlabel{^_1}{3}
\newpmemlabel{^_2}{3}
\newpmemlabel{^_3}{3}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Average (n = 10) f1-scores for test datasets as a function of the negative:positive class ratio.}}{3}{paragraph*.12}}
\newlabel{fig:marginfig}{{1}{3}{Sampling}{paragraph*.12}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Hyperparameters for random forest algorithm. The \textbf  {bold*} values mark the result of the optimisation by a 3 fold cross-validation procedure.}}{4}{paragraph*.14}}
\newlabel{RF-HP}{{3}{4}{Random Forest}{paragraph*.14}{}}
\@writefile{toc}{\contentsline {subsection}{Alternative approaches}{4}{subsection*.13}}
\@writefile{toc}{\contentsline {paragraph}{Random Forest}{4}{paragraph*.14}}
\newpmemlabel{^_4}{4}
\@writefile{toc}{\contentsline {paragraph}{Feature selection}{4}{paragraph*.15}}
\@writefile{toc}{\contentsline {paragraph}{New features and lexicons}{4}{paragraph*.16}}
\newpmemlabel{^_5}{4}
\@writefile{toc}{\contentsline {subsection}{Understanding the error}{4}{subsection*.17}}
\@writefile{toc}{\contentsline {paragraph}{Shifting error}{4}{paragraph*.18}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Pairplot of the sentiment score of tweets in the balanced sample for 4 emotions out of 8.}}{5}{paragraph*.16}}
\newlabel{sentiment}{{2}{5}{New features and lexicons}{paragraph*.16}{}}
\newpmemlabel{^_6}{5}
\newpmemlabel{^_7}{5}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Hatefulness (0: non-hateful, 1: hateful) scores of all features in the training set.}}{6}{paragraph*.18}}
\newlabel{fig:marginfig}{{3}{6}{Shifting error}{paragraph*.18}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Hatefulness (0: non-hateful, 1: hateful) scores of ambiguous features ($0 < hatefulness < 1$). Black dots mark the hatefulness computed for the whole dataset, the red dots represent the hatefulness of the same features computed with a balanced dataset (negative:positive = 1). All points above the dark point-line are feature that had their hatefulness under-estimated due to the unbalance distribution.}}{6}{paragraph*.18}}
\newlabel{fig:marginfig}{{4}{6}{Shifting error}{paragraph*.18}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Test set confusion matrix of the two rounds model (NB + RF) fitted with unbalanced dataset then ambiguous dataset.}}{7}{subsection*.19}}
\newlabel{final}{{4}{7}{Constructing a hybrid approach}{subsection*.19}{}}
\@writefile{toc}{\contentsline {subsection}{Constructing a hybrid approach}{7}{subsection*.19}}
\newpmemlabel{^_8}{7}
\@writefile{toc}{\contentsline {section}{Conclusion}{7}{section*.20}}
\bibdata{daimler}
\bibstyle{plainnat}
\newpmemlabel{^_9}{8}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces Summary table of f1-scores and sample properties of the approaches tested. The sampling neg:pos ratio is the relative ratio between the number of observations in each class. The hybrid approach uses two different samples (that overlap) to fit the two classifiers. All model but the first where fitted with lemmatised data. Notation: NB: Naive Bayes, RF: Random Forest, CV: cross-validation, MI: mutual Information criteria, est.: estimators, opti.: optimised by grid search. }}{8}{section*.20}}
\newlabel{scores}{{5}{8}{Conclusion}{section*.20}{}}
\ttl@finishall
